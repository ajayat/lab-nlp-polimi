{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka6H0qnGSpW9"
   },
   "source": [
    "# Transformer-based Text Classfication\n",
    "\n",
    "This notebook shows hot to make use of pre-trained Transformer models for building various types of text classifiers, some of which are multi-lingual. In particular, the notebook demonstrates the training of BERT-based text classifiers for:\n",
    "- categorising news group messages\n",
    "- detecting sentiment in Twitter messages\n",
    "- determining if a sentence paraphrases another.\n",
    "\n",
    "The notebook makes use of:\n",
    "- Transformer models from HuggingFace for training: https://github.com/huggingface/transformers\n",
    "- Lime for explanations: https://lime-ml.readthedocs.io/en/latest/index.html\n",
    "\n",
    "Much of the notebook is based on:\n",
    "- this Google Colab notebook: https://colab.research.google.com/drive/1YxcceZxsNlvK35pRURgbwvkgejXwFxUt\n",
    "- this notebook: https://github.com/amaiya/ktrain/blob/master/examples/text/MRPC-BERT.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLWptUEpvTfh"
   },
   "source": [
    "## Prepare environment\n",
    "\n",
    "**NOTE**: To run this notebook, you will **need a GPU** (Graphics Processing Unit) card\n",
    "- the notebook has been tested on Google Colab and you are advised to run it there\n",
    "- to use a GPU, you need to click on \"Runtime\" above, then \"Change runtime type\" and for \"Hardware accelerator\" choose the \"GPU\" option\n",
    "\n",
    "Now we should install and import all the dependencies, colab comes with all the libraries already pre-installed in the runtime, but datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-l_oq6HmglH"
   },
   "outputs": [],
   "source": [
    "#!pip3 install torch\n",
    "#!pip3 install -q transformers datasets\n",
    "#!pip3 install --upgrade scikit-learn==1.0.2\n",
    "#!pip3 install matplotlib\n",
    "#!pip3 install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afZupLAVWqhe"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66Q7pBASVbXD"
   },
   "source": [
    "You may need to restart the environment after installing the dependancies at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60uo8Ma8lruI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7w7V8Na_gPv2"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knkhmHlqgIjx"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxnegTgbXSum"
   },
   "source": [
    "Now let's check if our runtime is using a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buLDJl-hZFhr"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb1JvXwqXaeJ"
   },
   "source": [
    "`'cuda'`(Compute Unified Device Architecture) is an API developed by NVIDIA that allows software to utilize GPUs for general-purpose computing. So if torch is detecting a cuda device, it means it is detecting a GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdk5lPu3bxze"
   },
   "source": [
    "## Classification on the 20 Newsgroups dataset\n",
    "\n",
    "In the first example  \n",
    "\n",
    "Training on all of the documents in the dataset will take too long (a few hours), so we'll use just 4 today.\n",
    "- If you want to try on the full dataset, just remove the 'categories' attribute from the commands below.\n",
    "- Be warned though, that training time will be MUCH longer if you include all of the data, so first try subset of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rSbSqApYPBe"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['comp.graphics', 'rec.sport.baseball', 'sci.med', 'alt.atheism']\n",
    "train = fetch_20newsgroups(subset='train', shuffle=True, categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=True, categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xibNw613s4or"
   },
   "source": [
    "Print out some basic information about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCyuWfrDtHn7"
   },
   "outputs": [],
   "source": [
    "print('# train instances: ', len(train.data))\n",
    "print('# test instances:  ', len(test.data))\n",
    "print('classes: ', train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8-uo_WnVbXE"
   },
   "source": [
    "Let's compute the occurrences of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "461i8_nzVbXE"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(train.target)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XOMhW8X1j-O"
   },
   "source": [
    "A class count shows that there is a relatively even distribution of messages across the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VJnzlUStdLl"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(counts.keys(), counts.values(), color=\"#3F5D7D\", width=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaGNEu7FVbXF"
   },
   "source": [
    "### Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pe5xxVPrb4IO"
   },
   "source": [
    "#### Creating a model and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rctt-zMfkhME"
   },
   "source": [
    "For the moment we will use the standard pretrained Multilingual BERT model. Note that this **model is big**!\n",
    "- it contains 110 million parameters\n",
    "- it represents words (subword tokens to be precise) with embeddings of size 768 <-- compare this with the 50 dimensions we used with GloVe\n",
    "- it contains 12 self-attention layers (each with 12 heads) stacked on top each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-MljyocSpXG"
   },
   "outputs": [],
   "source": [
    "model_name = 'bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaS6XnAqSpXG"
   },
   "source": [
    "There are many other pretrained models availalable here: https://huggingface.co/transformers/pretrained_models.html\n",
    "- Give them a try and see which ones are faster and/or more stable to train!\n",
    "\n",
    "Load the transformer setting the maximum text length to 500 and the classes to be the four newsgroup categories mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myYmdpaVi6XB"
   },
   "outputs": [],
   "source": [
    "bert = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0Kn0RMEg1ZN"
   },
   "source": [
    "Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1rV7KHBwZA9"
   },
   "outputs": [],
   "source": [
    "bert = bert.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuk_C-3zaDeX"
   },
   "source": [
    "Now that we've downloaeded the base Multiligual BERT model, let's have a look at the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnHq1_iZYdSG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwIeFrFNjgf9"
   },
   "source": [
    "Check which of the parameters are trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGGOvNFchCNB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    print(param.size(), param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QGePPmboBHT"
   },
   "source": [
    "Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUbB1gjAoDnE"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yp4ju07NsN24"
   },
   "source": [
    "Now we can preprocess the training (and test) data using the transformer's data set API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBfcR-j-hjWI"
   },
   "source": [
    "First create a list of dictionaries with all the samples in the train and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Pw8W1OIrK6R"
   },
   "outputs": [],
   "source": [
    "train_data = [{'text': txt, 'label': lbl} for txt, lbl in zip(train.data, train.target)]\n",
    "test_data = [{'text': txt, 'label': lbl} for txt, lbl in zip(test.data, test.target)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32C0MMkLhr2g"
   },
   "source": [
    "Convert the data sets into the Huggingface data set API format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvpGAb7us2ij"
   },
   "outputs": [],
   "source": [
    "train_data = Dataset.from_list(train_data)\n",
    "test_data = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTYfG4twhyBL"
   },
   "source": [
    "Create a container with all the splits of the data set\n",
    "\n",
    "**Note**: We are using the same data for validation and testing, this is not a good practice, it's just for the tutorial, in general you should have separate data for validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSxUkZy6tcew"
   },
   "outputs": [],
   "source": [
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['validation'] = test_data\n",
    "data['test'] = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iM1xuM2th2SL"
   },
   "source": [
    "Finally use the tokenizer to convert the input strings into sequences of tokens (more on this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPOAbmdwsMaG"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jDlmkQAfFNI"
   },
   "source": [
    "Let's have a quick look at what the tokenised data looks like:\n",
    "- First print out the text of the first document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZVwnMckfgba"
   },
   "outputs": [],
   "source": [
    "print(train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJc8ABEBSpXH"
   },
   "source": [
    "Now print out the result of preprocessing the first document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0NH7iNW-mct"
   },
   "source": [
    "Each sample in tokenized_data is a dictionary with these keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1vSOEDikIHo"
   },
   "outputs": [],
   "source": [
    "tokenized_data['train'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0kMFd68-ui9"
   },
   "source": [
    "Let's inspect the input_ids and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xO_FOkTdSpXH"
   },
   "outputs": [],
   "source": [
    "print(tokenized_data['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RFVfwwOG3Vz"
   },
   "outputs": [],
   "source": [
    "print(tokenized_data['train'][0]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VolZFjl9f61n"
   },
   "source": [
    "Each integer above is the index of a token from the vocabulary of the BERT model.\n",
    "- How big is the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zYUTkJUgatH"
   },
   "outputs": [],
   "source": [
    "print(\"vocabulary size: \", len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXZgNOrmgoTJ"
   },
   "source": [
    "This vocabulary is relatively small compared to many word embeddings discussed in the lecture, which often reach over 1 million distinct words.\n",
    "- The vocab size is kept relatively small through the use of sub-word tokens that are found using a byte-pair encoding scheme.\n",
    "- We can take a single string and run it through the tokenizer to see the tokens produced as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LP3gdwRe1AVL"
   },
   "outputs": [],
   "source": [
    "example_text = \"I always wondered what those Transformer models were doing to my text. Here is a OutOfVocabWord.\"\n",
    "encoded_text = tokenizer(example_text).input_ids\n",
    "vocab_terms = list(tokenizer.vocab.keys())\n",
    "vocab_index = list(tokenizer.vocab.values())\n",
    "print(\"input text: \"+example_text)\n",
    "print(\"tokenized:  \", [vocab_terms[vocab_index.index(i)] for i in encoded_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQRSfSWWhFnF"
   },
   "source": [
    "Note that:\n",
    "- a '[CLS]' token has been added to the start of the tokenized text and a '[SEP]' token to the end.\n",
    "- BERT works as a masked autoencoder, meaning that it is trained to produce the same terms as output that are presented in input, including those that were masked out.\n",
    "- the '[CLS]' token is simply a special mask for the unknown class label that needs to be produced at the output.\n",
    "- the Byte-Pair Encoding has removed the suffixes from certain words (e.g. 'wondered' lost the 'ed'), but not all of them ('models' kept the 's')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4sGPJgOcBTd"
   },
   "source": [
    "#### Training the model\n",
    "\n",
    "Now we're ready to start training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gO6NOLH2qIOP"
   },
   "source": [
    "Prepare training arguments (including a name for the trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34ex9VhK7G7a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyLEhf2kvlxc"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"cool_trainer_name\",\n",
    "    per_device_train_batch_size=16,\n",
    "    report_to=None,  # Disabling wandb callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUPaP_OPjmFd"
   },
   "source": [
    "Disable weights in model encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Um37Z8bLwwHg"
   },
   "outputs": [],
   "source": [
    " for param in bert.bert.parameters():\n",
    "     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ewvcli7wkAWb"
   },
   "source": [
    "Build the traininer using the Huggingface trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doJDqHl6opqa"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pji9wH6rkLfl"
   },
   "source": [
    "Finally, run the training process (we train for three epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVm_LxBEopvX"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ho6eSo9IcI3_"
   },
   "source": [
    "#### Evaluating the model\n",
    "\n",
    "Now that the model has finished training, let's evaluate in on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX8NZVwVktii"
   },
   "source": [
    "First we can run the evaluation process included with the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zB1pCPU10Jwd"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8AV-Qi9k7Te"
   },
   "source": [
    "Then we can get the predictions on the test set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPoK1yoq1IRf"
   },
   "outputs": [],
   "source": [
    "preds = trainer.predict(tokenized_data['test'])\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojpVT2tEmh0S"
   },
   "source": [
    "Convert predicted logits to classes using the $\\mathrm{argmax}$ operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tjOw1YYmSHI"
   },
   "outputs": [],
   "source": [
    "y_pred = torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBNOiC6iuQYg"
   },
   "source": [
    "Get target label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CmMFBNouPA-"
   },
   "outputs": [],
   "source": [
    "label_names = test.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTyjAzeak_uV"
   },
   "source": [
    "... and use them to preprare a classification report using Scikit-Learn API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvcxCvLOcOje"
   },
   "outputs": [],
   "source": [
    "print(classification_report(test.target, y_pred, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHRb1I-JlIMF"
   },
   "source": [
    "... or compute a confusion matrix using the Scikit-Learn API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73YiamzdlI6t"
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(test.target, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhG3fPtPcVKe"
   },
   "source": [
    "We can have a look at the test examples on which the model made the worst predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzX6OeNZqYIu"
   },
   "source": [
    "Compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCABLebacTWM"
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.cross_entropy(torch.tensor(preds.predictions), torch.tensor(preds.label_ids), reduction='none')\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j_ACNkMqcKL"
   },
   "source": [
    "Identify the index of the element with the highest loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4-Ts24aqWQW"
   },
   "outputs": [],
   "source": [
    "index_of_instance = torch.argmax(loss).item()\n",
    "index_of_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymQd8xY3qugI"
   },
   "source": [
    "Loss of the corresponding sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJoHPh7VqWbN"
   },
   "outputs": [],
   "source": [
    "loss[index_of_instance].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjuo7nRcq7Bm"
   },
   "source": [
    "Predicted class and target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXWwP5IDq-JP"
   },
   "outputs": [],
   "source": [
    "print(f'Target class:    {label_names[test.target[index_of_instance]]}')\n",
    "print(f'Predicted class: {label_names[y_pred[index_of_instance]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJG_H42_b3ra"
   },
   "source": [
    "Print out the post with the top loss (by changing INDEX_OF_INSTANCE below) to try to find out why it is confusing the classfier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHYRBdBycfne"
   },
   "outputs": [],
   "source": [
    "print(test.data[index_of_instance])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcZQ6HbqdMcF"
   },
   "source": [
    "### Making predictions on new data\n",
    "\n",
    "We can run the trained classification model on new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTptGrh5VbXM"
   },
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpnQiMKyrbvd"
   },
   "source": [
    "Define a simple function to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Sqrv83frb5Z"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(text):\n",
    "    input_encodings = tokenizer(text, return_tensors='pt').to(device)\n",
    "    outputs = bert(**input_encodings)\n",
    "\n",
    "    lbl = label_names[torch.argmax(outputs.logits).item()]\n",
    "\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxQIB1-0rcCv"
   },
   "source": [
    "Define  simple function to predict the classes probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUYdUWthrcMz"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_proba(text):\n",
    "    input_encodings = tokenizer(text, return_tensors='pt').to(device)\n",
    "    outputs = bert(**input_encodings)\n",
    "\n",
    "    proba = torch.softmax(outputs.logits, dim=1).cpu().squeeze().numpy()\n",
    "\n",
    "\n",
    "    return dict(zip(label_names, proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyjLvuMus_fw"
   },
   "source": [
    "We can now try the trained model on new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hp8tw3Y0cnJa"
   },
   "outputs": [],
   "source": [
    "sample_text = 'Both of the home runs hit by Fernando Tatís in the third inning for the St. Louis Cardinals on April 23, 1999, were grand slams.'\n",
    "predict(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYqzU7nJhROw"
   },
   "source": [
    "We can see the probabilities assigned by the model to each of the classes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHT1jVTGi0yJ"
   },
   "outputs": [],
   "source": [
    "predict_proba(sample_text).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2p7Xojzi8uB"
   },
   "source": [
    "Where the classes are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg93RQvMjAFd"
   },
   "outputs": [],
   "source": [
    "predict_proba(sample_text).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H3TOwIgSpXL"
   },
   "source": [
    "OK, so the model appears to work well on English text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRmWpMvoiPha"
   },
   "source": [
    "#### Multi-lingual examples\n",
    "\n",
    "Let's try on a phrase in another language (Italian).\n",
    "- Note that the news group data we used to train the model is in English only, so the model has never seen any examples of news group messages in Italian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZOeu9cDdguM"
   },
   "outputs": [],
   "source": [
    "italian_text = 'I tasti su e giù sul mio portatile non funzionano più, quindi non posso più usarlo per giocare ;-('\n",
    "predict(italian_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiCLME7Dnoye"
   },
   "outputs": [],
   "source": [
    "predict_proba(italian_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWcw5VGIinK5"
   },
   "source": [
    "Did it get the prediction right?\n",
    "- Note that while the characer set for Italian and English is almost the same, there weren't any words (except perhaps for \"non\") in the above message that would have occurred in the training data.\n",
    "\n",
    "Let's try a message in a different character set:\n",
    "- Translate the text into Chinese using Google Translate: https://translate.google.com/\n",
    "- Then copy the text below and rerun the prediction.\n",
    "- Does it still work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuMmx8f5dr45"
   },
   "outputs": [],
   "source": [
    "chinese_text = '笔记本电脑上的向上和向下键不再起作用，因此我无法再使用它来播放;-(' #'INSERT CHINESE TEXT HERE'\n",
    "predict(chinese_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OiNy8look-l"
   },
   "outputs": [],
   "source": [
    "russian_text = \"Клавиши «вверх» и «вниз» на моем ноутбуке больше не работают, поэтому я больше не могу использовать их для игры ;-(\"\n",
    "predict(russian_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1r31KjnjU2m"
   },
   "source": [
    "### Explaining the predictions\n",
    "\n",
    "Let's try another piece of text, this time in English, but talking about a politician who wasn't a politician (but rather a celebrity) at the time in which the 20 News Groups dataset was created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXJ9qVdIjmlI"
   },
   "outputs": [],
   "source": [
    "sample_text = 'Donald Trump is the greatest living ex-president in the history of living ex-presidents. He\\'s also a bad golfer.'\n",
    "predict(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tHos7V6d8RQ"
   },
   "source": [
    "Let's invoke the `explain` method to see which words contribute most to the classification.\n",
    "\n",
    "We will need to install the **eli5** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIuKr0wk-FcX"
   },
   "outputs": [],
   "source": [
    "!pip3 uninstall flask\n",
    "!pip3 install eli5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Rw_-j2wVbXN"
   },
   "source": [
    "We need an additional apssage to make ELI5 work in this environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuG3pSRRVbXN"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "def monkeypath_itemfreq(sampler_indices):\n",
    "   return zip(*np.unique(sampler_indices, return_counts=True))\n",
    "\n",
    "scipy.stats.itemfreq=monkeypath_itemfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjruXVE47G9d"
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NV3GfJCLl2sq"
   },
   "source": [
    "Define function to output the probability distribution as a matrix `(n_samples, n_classes)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfYGGX-mlYnS"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_proba_values(text):\n",
    "    input_encodings = tokenizer(text, return_tensors='pt', padding=True).to(device)\n",
    "    outputs = bert(**input_encodings)\n",
    "\n",
    "    proba = torch.softmax(outputs.logits, dim=1).cpu().squeeze().numpy()\n",
    "\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWR5fodljOV5"
   },
   "source": [
    "Run the explainer (based on LIME) to see which words in the sentence are most important for the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQxI9hUK59oc"
   },
   "outputs": [],
   "source": [
    "# See https://amaiya.github.io/ktrain/text/predictor.html\n",
    "# See https://eli5.readthedocs.io/en/latest/autodocs/lime.html\n",
    "def explain(text, truncate_len=512, all_targets=False, n_samples=2500):\n",
    "    prediction = [predict(text)] if not all_targets else None\n",
    "    text = \" \".join(text.split()[:truncate_len])\n",
    "    te = TextExplainer(random_state=42, n_samples=n_samples)\n",
    "    _ = te.fit(text, predict_proba_values)\n",
    "    return te.show_prediction(\n",
    "        target_names=label_names, targets=prediction\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HgZDLYUeVaM"
   },
   "outputs": [],
   "source": [
    "explain(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcph2bSLe5cW"
   },
   "source": [
    "The words in the darkest shade of green contribute most to the classification.\n",
    "- Do they agree with what you would have expected for this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wceyWpdC-C1H"
   },
   "source": [
    "### Inspecting the Model\n",
    "\n",
    "Let's have a look at the architecture of the model that we have used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISHuI-fpSpXT"
   },
   "outputs": [],
   "source": [
    "print(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03oHrJjZvVxT"
   },
   "source": [
    "Compute the total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgL3V56lvWGD"
   },
   "outputs": [],
   "source": [
    "n_params = sum(param.numel() for param in bert.parameters())\n",
    "n_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erGkWyP_WJWg"
   },
   "source": [
    "The model contains over 100 million parameters consists of:\n",
    "- BERT (which contains alost all the parameters),\n",
    "- a drop-out layer (that is inserted to prevent overfitting), and\n",
    "- a dense (feedforward) layer to map the BERT embedding of size 768 to the 2 output neurons (representing the positive and negative classes).\n",
    "\n",
    "Let's print out information on the configuration of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiz0p_Zo7eB8"
   },
   "outputs": [],
   "source": [
    "bert.bert.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-s-YjRXLmXo6"
   },
   "source": [
    "### Saving and loading a fine-tuned model\n",
    "\n",
    "We can save predictor for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1nzxI_Jec-5"
   },
   "outputs": [],
   "source": [
    "bert.save_pretrained('/tmp/my_predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FL_ImfZ2Bf1M"
   },
   "source": [
    "Reload the predictor and use it to predict the class of a new piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDEU2s03fHsw"
   },
   "outputs": [],
   "source": [
    "bert = AutoModelForSequenceClassification.from_pretrained('/tmp/my_predictor').to(device)\n",
    "predict('My computer monitor is really blurry.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pP1w3-evVbXO"
   },
   "source": [
    "## Classifying the sentiment of Twitter messages\n",
    "\n",
    "We'll now train a different model to detect sentiment on Twitter data as we did for the previous tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruBhYoj-VbXO"
   },
   "outputs": [],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymw202nyVbXO"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "import re\n",
    "emoticon_regex = '(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)'\n",
    "positive_tweets_noemoticons = [re.sub(emoticon_regex,'',tweet) for tweet in positive_tweets]\n",
    "negative_tweets_noemoticons = [re.sub(emoticon_regex,'',tweet) for tweet in negative_tweets]\n",
    "\n",
    "tweets_x = positive_tweets_noemoticons + negative_tweets_noemoticons\n",
    "tweets_y = ['positive']*len(positive_tweets) + ['negative']*len(negative_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9aeDEYiVbXO"
   },
   "source": [
    "Use scikit-learn to split the data into training, validation and test sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbK6cSjnVbXO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "temp_x, test_x, temp_y, test_y = train_test_split(tweets_x, tweets_y, test_size=0.2)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(temp_x, temp_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5Dfpg0rVbXP"
   },
   "source": [
    "Now **repeat** all the steps from before for the twitter data:\n",
    "\n",
    "1.   Load the model (this time we'll try DistilBERT, which is a smaller transformer model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTcx744GVbXP"
   },
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "bert = bert = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "bert = bert.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBlXuZ5lVbXP"
   },
   "source": [
    "2. Process the training/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krifJ2oXVbXP"
   },
   "outputs": [],
   "source": [
    "train_data = [{'text': txt, 'label': 0 if lbl == 'negative' else 1} for txt, lbl in zip(train_x, train_y)]\n",
    "validation_data = [{'text': txt, 'label': 0 if lbl == 'negative' else 1} for txt, lbl in zip(valid_x, valid_y)]\n",
    "test_data = [{'text': txt, 'label': 0 if lbl == 'negative' else 1} for txt, lbl in zip(test_x, test_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxXvpvOjVbXP"
   },
   "outputs": [],
   "source": [
    "train_data = Dataset.from_list(train_data)\n",
    "validation_data = Dataset.from_list(validation_data)\n",
    "test_data = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKqZSBtSVbXP"
   },
   "source": [
    "3. Create a data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCfVXm1QVbXP"
   },
   "outputs": [],
   "source": [
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['validation'] = validation_data\n",
    "data['test'] = test_data\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True, batch_size=6400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6ulXAYiVbXP"
   },
   "source": [
    "4.   Train the model (We'll run for one epoch to make it faster, but it would be better to run for 4 or more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jd2ZIj_lVbXP"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"cool_trainer_name_1\",\n",
    "    per_device_train_batch_size=16,\n",
    "    report_to=None,  # Disabling wandb callbacks\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5hxRHv7VbXP"
   },
   "source": [
    "Possibly disable wights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekOTnlfxVbXP"
   },
   "outputs": [],
   "source": [
    " #for param in bert.base_model.parameters():\n",
    " #    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GS7GyqQLVbXP"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-zanL4LVbXP"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lc0Bz79YVbXP"
   },
   "source": [
    "5.   Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5iS7aIvVbXP"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OH7ZuIWGVbXP"
   },
   "outputs": [],
   "source": [
    "preds = trainer.predict(tokenized_data['test'])\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu-CguAIVbXP"
   },
   "source": [
    "Convert predicted logits to classes using the $\\mathrm{argmax}$ operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fATwk9nTVbXP"
   },
   "outputs": [],
   "source": [
    "y_pred = torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVe7HumJVbXP"
   },
   "source": [
    "Get target label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfU41IKZVbXP"
   },
   "outputs": [],
   "source": [
    "label_names = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juPrc5R3VbXP"
   },
   "source": [
    "... and use them to preprare a classification report using Scikit-Learn API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0MNJc-KVbXP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report([0 if lbl == 'negative' else 1 for lbl in test_y], y_pred, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0Qo8H5aVbXQ"
   },
   "source": [
    "6.   Make some predictions with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DmcvuxFVbXQ"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(text):\n",
    "    input_encodings = tokenizer(text, return_tensors='pt').to(device)\n",
    "    outputs = bert(**input_encodings)\n",
    "\n",
    "    lbl = label_names[torch.argmax(outputs.logits).item()]\n",
    "\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f52LxLm3VbXQ"
   },
   "outputs": [],
   "source": [
    "# predict('Had a lot of fun today!')\n",
    "predict('Today was really hard to give the lecture with the broken code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ebf9pfqeVbXQ"
   },
   "source": [
    "Did it work?\n",
    "\n",
    "Let's print out some information about the architecture of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Fo6pt-1VbXQ"
   },
   "outputs": [],
   "source": [
    "print(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kebL7mz9VbXQ"
   },
   "source": [
    "We see that the distilbert model has fewer parameters than the BERT model we used previously. It's interesting to see the pre-classifier layer in this model that maps the output embeddings from the transformer into a second embedding space (of the same size) that is then used as features for the final classification layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqhlQm_UVbXQ"
   },
   "source": [
    "## Pairwise classification task\n",
    "\n",
    "We now use the Microsoft Research Paraphrase Corpus (MRPC) to build a classifier for detecting paraphrased sentences.\n",
    "- This is an example of a paired sentence classfication task, where the prediction model takes in a **pair of texts as input** and produces a **binary label as output**.\n",
    "\n",
    "We'll first use pandas (Python's data analysis library) to download and parse the tab-separated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyVVy9ggVbXQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "train_file = 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt'\n",
    "test_file = 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt'\n",
    "train_df = pd.read_csv(train_file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "test_df = pd.read_csv(test_file, delimiter='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snOggBSUVbXQ"
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuLW6jfuVbXQ"
   },
   "source": [
    "We need to format the training data in the format needed for ktrain\n",
    "- so select the two columns containing strings\n",
    "- and get their values (as an 2d array)\n",
    "- then convert the array to a list of tuples (str,str) as required by ktrain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ea5nR1axVbXQ"
   },
   "outputs": [],
   "source": [
    "x_train = train_df[['#1 String', '#2 String']].values\n",
    "x_train = list(map(tuple, x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_MzEywQVbXQ"
   },
   "source": [
    "Repeat for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MkJRe1uVbXQ"
   },
   "outputs": [],
   "source": [
    "x_test = test_df[['#1 String', '#2 String']].values\n",
    "x_test = list(map(tuple, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVrsMNKmVbXQ"
   },
   "source": [
    "And of course, we need also the labels for the training / test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esLqoZ3nVbXQ"
   },
   "outputs": [],
   "source": [
    "y_train = train_df['Quality'].values\n",
    "y_test = test_df['Quality'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9IZTy9AVbXQ"
   },
   "source": [
    "Print out sizes of dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pDQqmDUVbXQ"
   },
   "outputs": [],
   "source": [
    "print(\"# train instances: \",len(x_train))\n",
    "print(\"# test instances:  \",len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN4_28bZVbXQ"
   },
   "source": [
    "Have a look at a random sentence pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_j_PfBQVbXQ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randint(0,len(x_train)-1)\n",
    "print(\"Index:      \"+str(i))\n",
    "print(\"Sentance 1: \"+x_train[i][0])\n",
    "print(\"Sentence 2: \"+x_train[i][1])\n",
    "print(\"Label:      \"+str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8rUXbabVbXQ"
   },
   "source": [
    "1. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWH-vY0tVbXR"
   },
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'  # 'bert-large-uncased'\n",
    "bert = bert = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "bert = bert.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4XggNN4VbXR"
   },
   "source": [
    "2. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nf9HGky7VbXR"
   },
   "outputs": [],
   "source": [
    "train_data = [{'sent1': txt[0], 'sent2': txt[1], 'label': lbl} for txt, lbl in zip(x_train, y_train)]\n",
    "test_data = [{'sent1': txt[0], 'sent2': txt[1], 'label': lbl} for txt, lbl in zip(x_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-jV8CDzVbXR"
   },
   "outputs": [],
   "source": [
    "train_data = Dataset.from_list(train_data)\n",
    "test_data = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EErMkKgzVbXR"
   },
   "source": [
    "Here we need to change a bit the tokenisation step, because we need to tokenise the two sentences together separated byt the special '\\[SEP\\]' token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jom4D5jqVbXR"
   },
   "outputs": [],
   "source": [
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['validation'] = test_data\n",
    "data['test'] = test_data\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sent1\"], examples[\"sent2\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True, batch_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9B9Idvp8VbXR"
   },
   "source": [
    "How do the samples look like?\n",
    "Let's consider the sentence pair we extacted randomly before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFeEiGWSVbXR"
   },
   "outputs": [],
   "source": [
    "sentence1 = tokenized_data['train'][i][\"sent1\"]\n",
    "sentence1 = tokenized_data['train'][i][\"sent2\"]\n",
    "encoded_text = tokenized_data['train'][i][\"input_ids\"]\n",
    "vocab_terms = list(tokenizer.vocab.keys())\n",
    "vocab_index = list(tokenizer.vocab.values())\n",
    "print(\"Sentance 1: \"+sentence1)\n",
    "print(\"Sentence 2: \"+sentence1)\n",
    "print(\"tokenized:  \", [vocab_terms[vocab_index.index(i)] for i in encoded_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0teDCfMkVbXR"
   },
   "source": [
    "2. Build and train a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFiToe9CVbXR"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"cool_trainer_name\",\n",
    "    per_device_train_batch_size=16,\n",
    "    report_to=None,  # Disabling wandb callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OImxsQhVbXR"
   },
   "source": [
    "Possibly disable wights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2xdKQtEVbXR"
   },
   "outputs": [],
   "source": [
    "#for param in bert.base_model.parameters():\n",
    "#    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEqmFmUUVbXR"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBXcoVeWVbXR"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2k8gxgfVbXR"
   },
   "source": [
    "3. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaV5ptI4VbXR"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkGTsiqrVbXR"
   },
   "outputs": [],
   "source": [
    "preds = trainer.predict(tokenized_data['test'])\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuMBp6pGVbXR"
   },
   "source": [
    "Convert predicted logits to classes using the $\\mathrm{argmax}$ operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W10jjENzVbXR"
   },
   "outputs": [],
   "source": [
    "y_pred = torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HW93VBJNVbXR"
   },
   "source": [
    "Get target label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fxgi7q1oVbXR"
   },
   "outputs": [],
   "source": [
    "label_names = ['not-paraphrase', 'paraphrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZkHowqFVbXR"
   },
   "source": [
    "... and use them to preprare a classification report using Scikit-Learn API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nfvj2pqVVbXR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb3EUfy2VbXS"
   },
   "source": [
    "4. Test the model on a pair of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cs2LoyhdVbXS"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(sent1, sent2):\n",
    "    input_encodings = tokenizer(sent1, sent2, return_tensors='pt').to(device)\n",
    "    outputs = bert(**input_encodings)\n",
    "\n",
    "    lbl = label_names[torch.argmax(outputs.logits).item()]\n",
    "\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuN5ldY9VbXS"
   },
   "outputs": [],
   "source": [
    "predict(\"I 'm eating pizza today\", \"She's eating pizza today\")\n",
    "#predict(\"I think I will eat some pizza today\", \"My flight was delayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbeAIKD-VbXS"
   },
   "source": [
    "Did it work?\n",
    "- Try it out on some of your own sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Wpq7wvlVbXS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
